\chapter{Novel Work} \label{novelw}

In this section the work completed so far will be presented. 

We first look at the CLEF 2017 runs, these are the datasets we will use for applying and evaluating our stopping methods. We establish a baseline result (Oracle); this tells use the best performance we could possibly get for each run. We then move on to evaluate two new methods that we developed to stopping, a percentage cut-off and a similarity score cut-off approach.

Further work looks at a different approach to stopping using partition sampling. We use a portion of the document collection to observe the distribution of the data and then predict the remaining portion. We first apply a curve to our data and then move on to use a Poisson Process.

Finally we look at an idea for building upon the dataset we are using by extracting more information from a study's full text. We present some initial findings as to how possible it is to retrieve these texts through automated techniques.



\section{CLEF 2017 Runs} \label{clefRuns}

For the CLEF 2017 Technologically Assisted Reviews in Empirical Medicine \cite{Kanoulas12017}, participants were expected to submit runs for ranking documents. Participants were given complex boolean queries that could be used for extracting relevant information to rank the documents. These runs were later released for public access \footnote{https://github.com/CLEF-TAR/tar/tree/master/2017-TAR/participant-runs}

\begin{table}[!htbp]
\scalebox{0.75}{
\centering
\caption{The 6 runs sampled from the CLEF 2017 task.}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
Run                  & Description                                                                    & Reference                                     \\ \hline
Sheffield-run-2      & Sheffield-2 used tfidf similarity along with standard pre-processing           & \cite{Alharbi2017}           \\ \hline
Waterloo A-rank-cost & Waterloo used a baseline model implementation from the TREC Total Recall Track & \cite{cormack2017technology} \\ \hline
Waterloo B-rank-cost & -                                                                              & \cite{cormack2017technology} \\ \hline
auth run-1           & AUTH used a learning-to-rank approach and used both batch and active learning  & \cite{Anagnostou2017clef}    \\ \hline
auth run 2           & -                                                                              & \cite{Anagnostou2017clef}    \\ \hline
ntu run-1            & Used convolutional neural networks (CNN)                                       & \cite{lee2017clef}           \\ \hline
ucl full-text        & Used a deep learning model architecture                                        & \cite{singh2017identifying}  \\ \hline
\end{tabular}
}
\end{table}


We have taken 6 ranking sets of which are of different quality. The Waterloo and AUTH ranks are the best rankings followed by Sheffield. The UCL and NTU submissions feature poorer quality rankings. We tried to take runs that had a varying level of quality in the rankings.

CLEF 2017 runs will follow a format similar to the example below. The most important information being the topic id and the document id. 


\begin{tcolorbox} \label{scoreExample}
CD010775 NF 19307324 1 0.27152011529138564 Test-Data-Sheffield-run-2 
\end{tcolorbox}

Results can be evaluate using qrel files, supplied as part of the CLEF 2017 data. These files contain relevant documents for each topic, and are formatted as follows:

\begin{tcolorbox}
CD008803     0  21467181     0  \\
CD008803     0  20872357     1  \\
CD008803     0  23837966     0 
\end{tcolorbox}

Where the 1 or 0 on the right side indicates if the document is relevant for a study. Therefore the quality of the datasets can be observed by looking at how many of the relevant documents feature towards the top of the rankings.


\section{Baseline Approaches to Stopping} \label{baselineapp}

We can now establish some baseline approaches for finding stopping points. Approaches are heavily dependent on the initial rankings of the document collection, and naturally assume more relevant documents feature towards the start of the collection. 

\subsection{Oracle Scores} \label{oracleScores}

Looking at the oracle scores for each set of rankings will tell us the best we can possibly for do this task.  We will used the 70\% recall benchmark.

These scores are calculated as the minimum amount of effort that could be made to achieve 70\% recall. We first loop over each topic and update a count each time a relevant document is found. We then examine the recall each time the count is updated and determine if the threshold score has been reached (70\%). If 70\% has been reached we stop iterating over the rankings and calculate the effort made up to this point.

Results are taken as averages over all topics.

\begin{table}[H]

\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline
 Submission & recall & reliability & effort  \\ 
 Test\_Data\_Sheffield-run-2 & 0.7 &	1.0	&	0.11 \\ 
 Waterloo A-rank-cost & 0.7 &	1.0	&	0.07 \\ 
 Waterloo B-rank-cost & 0.7 &	1.0	&	0.06 \\ 
 auth run-1 & 0.7 &	1.0	&	0.08 \\ 
 auth run-2 & 0.7 &	1.0	&	0.09 \\ 
 ntu run-1 & 0.7 &	1.0	&	0.4 \\ 
 ucl full-text & 0.7 &	1.0	&	0.67 \\ 
 \hline
\end{tabular}

\caption{Lowest effort possible to find 70\% of relevant documents. }

\end{table}

These scores highlight the importance of the ranking methods for finding a stopping point. The best performer, Waterloo B-rank-cost needs just 6\% effort to hit 100\% reliability. The worst performer, ucl full-text requires 67\% effort for the same level of reliability.


\subsection{Percentage cut-off method} \label{perCutOffMethod}

As a first approach we could simply take a cut of the document collection and evaluate how many relevant documents we have retrieved. This likely to be very dependant on the initial rankings of the document collection. 

\begin{table}[!htbp]
\scalebox{0.75}{
\centering
\caption{Comparison of results between rankings when looking at a percentage of the ranked documents.}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\% of Documents      & \multicolumn{3}{l|}{10\%}     & \multicolumn{3}{l|}{25\%} & \multicolumn{3}{l|}{50\%} & \multicolumn{3}{l|}{75\%} & \multicolumn{3}{l|}{90\%} \\ \hline
Run                  & Recall & Reliability & Effort & -       & -      & -      & -       & -      & -      & -       & -      & -      & -       & -      & -      \\ \hline
Sheffield-run-2      & 0.49   & 0.16        & 0.10   & 0.74    & 0.66   & 0.25   & 0.91    & 0.93   & 0.50   & 0.98    & 1.00   & 0.75   & 0.99    & 1.00   & 0.90   \\ \hline
Waterloo A-rank-cost & 0.80   & 0.6         & 0.10   & 0.91    & 0.93   & 0.25   & 0.98    & 1.00   & 0.50   & 0.99    & 1.00   & 0.75   & 0.99    & 1.00   & 0.90   \\ \hline
Waterloo B-rank-cost & 0.73   & 0.63        & 0.10   & 0.90    & 0.93   & 0.25   & 0.98    & 1.00   & 0.50   & 0.99    & 1.00   & 0.75   & 0.99    & 1.00   & 0.90   \\ \hline
auth run-1           & 0.72   & 0.63        & 0.10   & 0.90    & 0.93   & 0.25   & 0.97    & 1.00   & 0.50   & 0.99    & 1.00   & 0.75   & 0.99    & 1.00   & 0.90   \\ \hline
auth run 2           & 0.68   & 0.60        & 0.10   & 0.88    & 0.9    & 0.25   & 0.97    & 1.00   & 0.50   & 0.99    & 1.00   & 0.75   & 0.99    & 1.00   & 0.90   \\ \hline
ntu run-1            & 0.19   & 0.00        & 0.10   & 0.39    & 0.06   & 0.25   & 0.62    & 0.43   & 0.50   & 0.83    & 0.83   & 0.75   & 0.91    & 0.93   & 0.90   \\ \hline
ucl full-text        & 0.08   & 0.00        & 0.10   & 0.22    & 0.00   & 0.25   & 0.46    & 0.03   & 0.50   & 0.70    & 0.70   & 0.75   & 0.87    & 0.93   & 0.90   \\ \hline
\end{tabular}
}
\end{table}

All scores are averaged over the entire ranking set.

Sheffield has an average recall of 0.49 by the time 10\% of the documents have been observed. Waterloo has achieved 80\% recall by this point. After looking through 25\% of the rankings Waterloo and AUTH have achieved around 90\% recall of documents.

The limitation of this approach is the effort required looking through documents is still high. We want to lower this effort as much as possible, whilst only having to observe relevant documents.


\subsection{Similarity score cut-off method} \label{simScoreMethod}

A similarity score method will assume each document in the rankings has a score associated with it. Consider the ranking format described in \ref{clefRuns}. The 5th column describes a similarity between the document and the query. Similarity scores gradually decline as we descend down the rankings.


For the Sheffield set of rankings, the similarity score comes from the cosine similarity between a vectorized query and document. 

The similarity score can be used derive a stopping point. This method works by looking at documents $D_i$ and $D_{i+1}$ and determining if the difference between the similarity scores has become too large. This method will work on the basis that documents that are no longer relevant will have a sudden drop in score such that we can identify this as our stopping point.

\begin{equation}
	  Difference(D_i, D_{i+1}) > C
\end{equation}

Where difference returns a score of how close document $D_i$ and $D_{i+1}$ are together and $C$ is a cut-off constant. We can expand this to an example:

\begin{equation}
	  (1 -(0.73 / 0.75)) * 100 > 0.015
\end{equation}

Here we are saying if the two documents' scores are above 1.5\% then we should stop looking down the rankings.


\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline
 $dif(D_i, D_{i+1})$ & recall & reliability & effort  \\ 
 0.01\% & 0.025 &	0.000	&	0.0023 \\ 
 0.05\% & 0.120 &	0.100	&	0.100 \\ 
 1\% & 0.359 &	0.333	&	0.333 \\ 
 2\% & 0.880 &	0.860	&	0.860 \\ 
 5\% & 1.000 &	1.000	&	1.0000 \\ 
 \hline
\end{tabular}

\caption{Similarity cut-off comparison for stopping for Test\_Data\_Sheffield-run-2. Using cosine similarity scores.}
\end{table}

These results are highly sensitive to the similarity score and show it is difficult to use this score as an effective measure for stopping. We found similarity scores rarely have sudden drops in values, making it difficult to use this method to identify a stopping point.

As an alternative approach, we can look at just the top document $D_1$, and compare it to the succeeding documents $D_{1+i}$ in the rankings. We can formulate this as follows:

\begin{equation}
	  Difference(D_1, D_{1 + i}) > C
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|} 
\hline
 $dif(D_1, D_{1+i})$ & recall & reliability & effort  \\ 
 10\% & 0.048 &	0.000	&	0.004 \\ 
 20\% & 0.063 &	0.000	&	0.007 \\ 
 30\% & 0.113 &	0.000	&	0.152 \\ 
 40\% & 0.191 &	0.030	&	0.029 \\ 
 50\% & 0.319 &	0.060	&	0.063 \\
 60\% & 0.460 &	0.200	&	0.113 \\ 
 70\% & 0.638 &	0.433	&	0.210 \\ 
 80\% & 0.841 &	0.800	&	0.387 \\ 
 90\% & 0.979 &	1.000	&	0.679 \\ 
 100\% &1.000 &	1.000	&	1.000 \\  
 \textbf{85.5\%} & \textbf{0.934} &	\textbf{1.000}	&	\textbf{0.538} \\  
 \hline
\end{tabular}

\caption{Similarity cut-off comparison for stopping for Test\_Data\_Sheffield-run-2. Using cosine similarity scores. Compares first document to succeeding documents}
\end{table}

We found results to be much better when using the first document to look for a stopping point. 85.5\% difference in first and subsequent documents was found to be the point for hitting 1.0 reliability. This comes at the expense of making just over 50\% effort.

\section{Sample Methods to Stopping} \label{samplemethods}

The limitations of the methods presented in \ref{baselineapp} is that they they still require looking through a large volume of documents.

The approaches in this section will use sampling methods. These approaches assumes we have sensible ranking algorithm for returning documents for a query. They work by generating a sample set, which acts as our model for predicting a stopping point. Therefore the quality of these methods will depend on the sparseness of the document rankings, making it challenging to establish a single stand-out method.

The first step for these methods is to generate a sample set. We used an interval method for generating our set, i.e select every $N$th document. The intuition being that the distribution of relevant document in the sample set, should be similar to that of the complete set of relevant documents. This makes it suitable to use as a model. 


\iffalse
A limitation of this sampling method is that for topics with very few documents it is easy for a sample to miss many of them. This creates a subset set bias, where one set contains a larger percentage of relevant documents. Consider a query that returns 10000 candidate documents of which only 10 have been pre-determined to be relevant. Its not too unlikely that a randomly sampled subset would contain 0 relevant documents. We can use the following equation to tell us how much information we can take from a pre-evaluated topic:

\begin{equation}
I = \frac{rel(T_i)}{|T_i|}
\end{equation}

Where $T_i$ is a given topic and $rel$ computes the number of relevant documents for that topic. Therefore $I$ is telling us how useful the topic is at fitting a curve. We can take the average simply by taking the mean of $I$ across all topics:

\begin{equation}
Usefullness = \frac{\sum{I}}{|T|}
\end{equation}
\fi


\subsection{Curve Fitting}

Our first approach is to fit a non-linear curve against a sample set. We used a sample size of 3. We opted for an exponential curve as the number of relevant documents found will increase as more documents are looked, but eventually level out as relevant documents become less frequent.  

\begin{equation}
F(x) = n - ae^{-kx}
\end{equation}

Where $a$, $k$ and $n$ are learnt weights and $x$ is an associated return rate for a document. We generate the curve using the non-linear least squares algorithm \cite{leastsqures}. 


\iffalse
We can visualise the curve along with the confidence intervals. The Y axis is the predicted number of relevant documents for the topic. X axis shows the true number of documents returned for the query. 

\begin{figure}[H]
\center
\includegraphics[width=10cm]{figures/curve_fit_example.png}
\caption{Example of fitting a curve for a topic using sampling}
\end{figure}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|} 

 \hline
 sample size & recall & reliability & effort  \\ 
 1 & 0.91 &	0.96	&	1 \\ 
 3 & 0.66 & 0.5	&	0.48 \\ 
 5 & 0.481 & 0.33	&	0.315 \\ 
 \hline
\end{tabular}
\caption{Comparison of different sample intervals against recall and effort. Ranking Method: Test\_Data\_Sheffield-run-2 \cite{Alharbi2017}}

\end{table}

\fi


\subsubsection{Curve Predictions}

\iffalse
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|} 

 \hline
 Submission & recall & reliability & effort & topics sampled  \\ 
 Test\_Data\_Sheffield-run-2 & 0.68 &	0.53	&	0.50 & 30 \\ 
 Waterloo A-rank-cost & 0.70 & 0.46	&	0.44 & 30 \\ 
 Waterloo B-rank-cost & 0.70 & 0.46	&	0.40 & 30 \\ 
 auth run-1 & 0.74 & 0.53	&	0.43 & 30 \\ 
 auth run-2 & 0.70 & 0.50	&	0.43 & 30 \\ 
 ntu run-1 & 0.71 & 0.43	&	0.68 & 30 \\ 
 ucl full-text & 0.86 & 0.8	&	0.91 & 30 \\ 
 \hline
\end{tabular}
\caption{Evaluation of curve fitting for different CLEF 2017 runs. Sample size = 3. Results are taken as averages over all topics for search method. No topic cut-off}

\end{table}
\fi


The number of topics was reduced down from 30 to 23 using a cut-off parameter. This reduces some of topics that contain fewer relevant documents and does not generate suitable prediction curves. We used a cut-off parameter of 0.5\%. This can be read as the proportion of relevant documents in a document collection must be atleast 0.5\%, which was found to be a suitable amount to give us a good volume of topics.

We also include a confidence interval evaluation for lower bounded range of a $3\sigma$ confidence interval. The key advantages of using a curve as  method of evaluating stopping criteria is being able to make use of this confidence interval in a real systematic review. In the context of a systematic reviewers at the data filtering stage, we could specify that the system is 95\% certain that 70\% of relevant documents have been found. At which point the reviewer can decide if its worth continuing to look at documents.

\begin{figure}[H]
\center
\includegraphics[width=17cm]{figures/cf_example.png}
\caption{Example of a prediction curve for topic CD008081. Confidence bars are included over $3\sigma$. Estimated point of hitting 70\% denoted by black point.}
\end{figure}

\begin{table}[H]
\scalebox{1}{
\centering
\begin{tabular}{|c|c|c|c|c|} 

 \hline
 Submission & recall-lower & reliability-lower & effort-lower & topics sampled  \\ 
 Test\_Data\_Sheffield-run-2 & 0.69 \, 0.74, &	0.52 \, 0.60	&	0.48 \, 0.51 & 23 \\ 
 Waterloo A-rank-cost & 0.71 \, 0.73, &	0.47 \, 0.47	&	0.43 \, 0.44 & 23 \\ 
 Waterloo B-rank-cost & 0.71 \, 0.75, &	0.52\, 0.82	&	0.41\, 0.43 & 23 \\ 
 auth run-1 & 0.72 \, 0.74, &	0.52 \, 0.60	&	0.41 \, 0.42 & 23 \\ 
 auth run-2 & 0.70 \, 0.72, &	0.52 \, 0.60	&	0.42 \, 0.43 & 23 \\ 
 ntu run-1 & 0.76 \, 0.74, &	0.56 \, 0.52	&	0.72 \, 0.70 & 23 \\ 
 ucl full-text & 0.86 \, 0.94, &	0.82 \, 0.86	&	0.91 \, 0.95 & 23 \\ 
 \hline
\end{tabular}
}
\caption{Evaluation of curve fitting for different CLEF 2017 runs. lower = lower-bound confidence interval. Sample size = 3. Results are taken as averages over all topics for search method. with 0.5\% cut-off}

\end{table}

We have deliberately compared two of the better participant rankings (Waterloo and auth) and two of the lower performers (ntu and ucl). We can see the quality of the initial rankings significantly influences the performance of our stopping criteria. This suggests there is a important relationship between using a curve to predict a stopping point and how good the initial ranking of documents is. 

Some of the datasets to produce curves due to the sparsity of relevant documents. In situations where this occurred, we returned everything for the given topic, resulting in 100\% recall at the expense of 100\% effort.

\iffalse
\subsection{Gaussian Process Fitting}

As an alternate approach to fitting a simple curve, we can apply a Gaussian Process (GP) \cite{ebden2008gaussian}. We will apply a constant kernel plus a squared-exponential kernel. GP was implemented using scikit learn classes. \footnote{http://scikit-learn.org/stable/modules/gaussian\_process.html}


\begin{table}[H]
\scalebox{1.0}{
\centering
\begin{tabular}{|c|c|c|c|c|} 

 \hline
 Submission & recall-lower & reliability-lower & effort-lower & topics sampled  \\ 
 Test\_Data\_Sheffield-run-2 & 0.73 \, 0.73, &	0.73 \, 0.73	&	0.50 \, 0.50 & 23 \\ 
 
 
 Waterloo A-rank-cost & 0.70 \, 0.70, &	0.40 \, 0.40	&	0.42 \, 0.42 & 23 \\ 
 
 
 Waterloo B-rank-cost & 0.73 \, 0.73, &	0.71 \, 0.71	&	0.41 \, 0.41 & 23 \\ 
 
 
 auth run-1 & 0.74 \, 0.75, &	0.56 \, 0.60	&	0.42 \, 0.42 & 23 \\ 
 
 
 auth run-2 & 0.72 \, 0.73, &	0.52 \, 0.52	&	0.42 \, 0.42 & 23 \\ 
 
 
 ntu run-1 & 0.67 \, 0.67, &	0.40 \, 0.40	&	0.64 \, 0.64 & 23 \\ 
 
 
 ucl full-text & 0.62 \, 0.62, &	0.52 \, 0.56	&	0.82 \, 0.82 & 23 \\ 
 \hline
\end{tabular}
}
\caption{Comparison of different of sample method using gp for different CLEF 2017 runs. lower = lower-bound confidence interval. Sample size = 3. Results are taken as averages over all topics for search method. with 0.5\% cut-off}

\end{table}


We found the GP is not truly representing the distribution of the data.

\begin{figure}[H]
\center
\includegraphics[width=12cm]{figures/GP_fit.png}
\caption{Visualisation of using a confidence interval for predicting a stopping point using a gp.}
\end{figure}


\subsection{Conclusion on Curve fitting and GP} \label{conclusCurveAndGp}

We implemented two methods for predicting stopping points in ranked medical studies. Our first approach used a general curve to estimate the point in which 70\% recall is likely to have been hit. Our second method used a Gaussian Process in the same way. We used a sampling method to generate our curves to makes predictions about the remaining studies.

\fi



\section{Poisson Process for Stopping Points} \label{window_samp}

A Poisson Process can be used to model points in time in which events occur. In this situation we wish to observe the rate in which a relevant document occurs in a collection of documents.

We can observe the relevant document distribution for each topic by plotting relevant document occurrences across the whole rankings.


\begin{figure}[H]
\center
\includegraphics[width=15cm]{figures/distrib.jpg}
\caption{Relevant document distribution over Sheffield dataset}
\end{figure}

The quality of the initial rankings will determine how many relevant documents occur towards the start of the distribution. Naturally the number of relevant documents decrease as we proceed down the rankings.

The Poisson distribution is a way for us to model the occurrences of relevant documents in a fixed time-frame, in our situation, the number of documents returned by the query. To estimate the overall rate of which relevant documents occur, we can observe how many relevant documents occur within a threshold.


\begin{equation}
	  \lambda = \frac{r_i}{|D|}
\end{equation}

Where $r_i$ is the number of relevant documents in a sample set and $|D|$ is the number of documents in the sample set.

Supposing we sample 10\% of the 1000 documents, of which 7 relevant documents occur:

\begin{equation}
	  \lambda = \frac{7}{100} = 0.07
\end{equation}

We can use the rate parameter to estimate the probability of there being atleast one relevant documents, after observing $n$ documents:

\begin{equation}
	  P = 1 - e ^ {0.7n}
\end{equation}


\begin{figure}[H]
\center
\includegraphics[width=10cm]{figures/probOneDoc.jpg}
\caption{Probability of seeing atleast one relevant document by sampling 10\% of documents for topic CD008081}
\end{figure}

The plot shows as we look at more documents, we are increasingly likely to have seen one relevant document. While this is useful to know, we can not use this as a method for predicting a suitable stopping point.

A Homogeneous Poisson process can be used to model the occurrences of relevant document and then used to predict the probability of there bring $r$ relevant documents after $n$ documents have been observed. 

\begin{equation}
	  P(r) = \frac{(\lambda n)^r}{r!} e ^ {-\lambda n}
\end{equation}


Due to the high likelihood that $r!$ will be exceeding large, we can use a stirling approximation, maintaining a similar value as to what we would have obtained computing the factorial.

\begin{equation}
	   r \approx \sqrt{2\pi r} \left( \frac{r}{e} \right) ^r
\end{equation}

By summing over the probability mass from for values of $n$ between 1 and the size of the document collection we can estimate at what point 95\% reliability (stopping point $s$) is reached. 

\begin{equation}
	   s = \sum_{0, i < 0.95}^{|n|} \frac{(\lambda i)^r}{stirling(r!)} e ^ {-\lambda i}
\end{equation}

The limitation of using an Homogeneous Poisson process is that the rate parameter is constant throughout distribution. This means if we looked at 10\% of the rankings the rate of relevant documents would be assumed to be constant for the remainder of the document collection. Therefore this method would only be suitable if we had a random relevance rate.


\begin{figure}[H]
\center
\includegraphics[width=16cm]{figures/overestimate.png}
\caption{Homogeneous Poisson process overestimating the rate of documents, resulting a prediction curve that would expect us to look at 1500 of the 2000 documents to reach 70\% recall}
\end{figure}

\subsection{Non-Homogeneous Poisson Process}

As the rate parameter is varies throughout the distribution of documents, we can use a Non-Homogeneous Poisson process. A Non-Homogeneous Poisson process is similar to an ordinary Poisson process, except that the average rate of arrivals is allowed to vary with time \cite{nonhomp}. 


\subsubsection{Non-Homogeneous Poisson Definition} \label{nn_pp_def}

We can use a non-homogenous poission process to estimate the number of relevant documents in a given interval by integrating the rate function $\lambda(x)$ across the interval.

We first define our intervals as $a$ and $b$ and integrate the values between them with respect to $x$

\begin{equation}
	   \int_a^b \lambda(x) d(x) = \Lambda(a, b)
\end{equation}

Therefore the probability of there being $r$ relevant documents in the interval $(a, b$)

\begin{equation}
	   P(N(a, b) = r) = \frac{[\Lambda(a,b)]^r}{r!} e^{-\Lambda(a,b)}
\end{equation}

We can make the assumption that the rate in which relevant documents appear is an exponential function.

\begin{equation}
	   \lambda(x) = ae^{kx}
\end{equation}

Therefore

\begin{equation}
	   \int \lambda(x) d(x) = \int ae^{kx} dx = \frac{a}{k}e^{kx}
\end{equation}

As we are only interested in knowing the total number of relevant documents, we assume that we are integrating from 0 to the total number of documents.

\begin{equation}
	   \Lambda(0, n) = \int_0^n ae^{kx}dx = \left[\frac{a}{k}e^{kx}\right]_0^n = \frac{a}{k}(e^{kn} - 1)
\end{equation}

So

\begin{equation}
	   P(N(0,n) = r) = \frac{(\Lambda(0,n))^r}{r!}e^{-\Lambda(0,n)} = \frac{\left(\frac{a}{k}(e^{kn} -1)\right)^r}{r!} e^{-(\frac{a}{k}(e^{kn} - 1)})
\end{equation}

Therefore given values for $a$ and $k$ which we can learn from fitting an exponential curve, we can predict the probability of there being $r$ relevant documents within the entire set of $n$ documents.


\subsubsection{Implementation Non-Homogeneous Poisson Process} \label{imp_non_hom}

\paragraph{Window Sampling} \label{window_samp}

It is useful for us to estimate the rate at which relevant documents occur. By iterating over each document in a returned set of documents, and evaluating the relevant documents in a given window, we can estimate this rate parameter. 

\begin{table}[H]

\centering
\begin{tabular}{|c|c|c|} 
\hline
 Document Rank & Relevant & RelScore  \\ 
 1 & Y &			1/3=0.33 \\ 
 2 & Y &			3/4=0.75 \\ 
 3 & N &			4/5=0.80 \\ 
 4 & Y &			3/5 =0.60 \\ 
 5 & Y &			2/5=0.40 \\ 
 6 & N &			2/5=0.40 \\ 
 7 & N &			1/4=0.25 \\
 8 & N &			0/3=0.00 \\
  
 \hline
\end{tabular}

\caption{Using window sampling with a window size of 2 either side.}

\end{table}

The challenge is being able to find an optimum sample size. A sample size that is too small will not provide enough information on the distribution of the surrounding documents. A sample size that is too large is a risk, as the if the documents rankings are sparse the rate estimations would not be as useful.

\paragraph{Poisson Process Steps} \label{pp_steps}

We created a Python-based implementation using scipy for the curve modelling. Our steps can be broken down as follows:

\begin{enumerate}
\item Load a run final into memory. For this task we used the Sheffield run data from CLEF 2017.

\item Create a frequency based distribution of the data for relevant documents, e.g 1, 1, 2, 3, 3, 3, 3, 4

\item Normalize the data for positions in which relevant documents occur e.g x = 1, 2, 3, 4, 5 y = 2, 12, 17, 34, 61

\item Use normalized data along with window sampling \ref{window_samp} to create a probabilistic relevant document frequency in a given window. 

\item Take a sample percentage from the distribution (start at 1\%). Use this percentage to fit an exponential curve ($ae^{kx}$) using non-linear least squares \footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html}}. This provides values for $a$ and $k$ which can then be substituted into equation 4.16 to give the distribution of the total number of relevant documents.

\item Use weights from curve fit and Non-Homogeneous Poisson process to estimate the number of relevant documents across the entire collection. For values from $r: 0 \mapsto n$ we will get a probability of $r$ relevant documents. Assuming 95\% of documents is acceptable, we can sum over the probability mass until this value is reached.

\end{enumerate}

\paragraph{Finding an Optimum Window Size} \label{pp_steps}

To find a general window size for sampling we will observe the sum of the mean squared error over all topics for sampled data from 10\% to 40\%. We will consider sample window sizes in the range of 1 to 100.  This will tell us how well our exponential curve is fitting to the data for each window sample size.

\begin{figure}[H]
\center
\includegraphics[width=15cm]{figures/sse_avg.png}
\caption{Comparison of different sample portions against sample window size}
\end{figure}

We can see a fairly steep drop decline between using a sample window of 1 to 20. As we reach 50, the curve has become almost flat. Therefore, we will use a sample window of 50 for the remainder of this work.


\paragraph{Results for Non-Homogeneous Poisson Process} \label{nn_pp_def}

Using the above definition and sample window we created a non-homogeneous Poisson process and generated some results. We still consider different sample portions for generating our initial curve.

\begin{table}[H]
\scalebox{0.65}{
\centering
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}
\hline
\% of Documents      & \multicolumn{3}{l|}{10\%}     & \multicolumn{3}{l|}{20\%} & \multicolumn{3}{l|}{40\%} & \multicolumn{3}{l|}{60\%} & \multicolumn{3}{l|}{80\%}  & \multicolumn{3}{l|}{90\%} \\ \hline
Run                  & Recall & Reliability & Effort & -       & -      & -      & -       & -      & -      & -       & -      & -      & -       & -      & -   & -       & -      & -      \\ \hline

Sheffield-run-2      & 0.92   & 0.86        & 0.76   & 0.97    & 1.0   & 0.85   & 0.99    & 1.0   & 0.96   & 1.0    & 1.0   & 0.99   & 1.0    & 1.0   & 0.99  & 1.0    & 1.0   & 0.99   \\ \hline

Waterloo A-rank-cost      & 0.89   & 0.86        & 0.72   & 0.96    & 1.0   & 0.86   & 0.99    & 1.0   & 0.92   & 0.99    & 1.0   & 0.96   & 0.99   & 1.0   & 0.97  & 0.99    & 1.0   & 0.98   \\ \hline

Waterloo B-rank-cost      & 0.90   & 0.82        & 0.77   & 0.92    & 1.0   & 0.87   & 0.98    & 1.0   & 0.92   & 0.99    & 1.0   & 0.96   & 0.99   & 1.0   & 0.98  & 0.99    & 1.0   & 0.98   \\ \hline

auth run-1     & 0.93   & 0.91        & 0.78   & 0.97    & 0.95   & 0.93   & 0.99    & 1.0   & 0.95   & 1.0    & 1.0   & 0.99   & 0.99   & 1.0   & 0.99  & 1.0    & 1.0   & 0.99   \\ \hline

auth run-2    & 0.91   & 0.95        & 0.70   & 0.96    & 0.95   & 0.87   & 0.99    & 1.0   & 0.93   & 1.0    & 1.0   & 0.99   & 1.0   & 1.0   & 0.99  & 1.0    & 1.0   & 0.99   \\ \hline

\end{tabular}
}
\caption{Comparison of results using a non-homogeneous Poisson process and different sample sizes}
\end{table}


\paragraph{Results for Non-Homogeneous Poisson Process using Dynamic Sample Size} \label{nn_pp_def_dy}

The limitation of using a set sample percentage is that it is not suitable for all topics. In some situations we may reach a desired level of recall sooner by sampling few documents. For this reason we alter our method slightly to increase the sample over an iteration until we estimate that we have reached our desired level of recall. We add two more steps to our process:

\begin{enumerate}

\item[7] Determine if estimated number of relevant documents across the entire collection has reached the desired level of recall. 

\item[8] Repeated steps 5-7 until condition 7 is satisfied.

\end{enumerate}

\begin{table}[H]

\centering
\begin{tabular}{|c|c|c|c|c|} 
 \hline
 Run & Recall & Reliability & Effort & Topics Ran  \\ 
 Sheffield-run-2 & 0.90 & 0.95 & 0.67 & 14/23 \\
 Waterloo A-rank-cost & 0.90 & 0.95 & 0.64 & 20/23 \\
 Waterloo B-rank-cost & 0.92 & 1.0 & 0.69 & 19/23 \\
 auth run-1 & 0.92 & 0.95 & 0.59 & 21/23 \\
 auth run-2 & 0.91 & 0.95 & 0.60 & 20/23 \\
 \hline
\end{tabular}
\caption{Using an iterative approach to a non homogeneous Poisson process. Scores are macro averages over all topics}

\end{table}

This imposes a further challenge in that a non-fitting curve may result in an estimation never reaching a deserved level of recall. After applying some data manipulation to the sampled data we were able to get the majority of the topics running using a non homogeneous Poisson process. The maximum possible topics that could be ran is 23. In situations where the topic could not generate a curve/the Poisson process was not able to predict a stopping point we applied an effort score to the topic (1.0). 

Overall, we still found this approach to be highly dependant on the quality of the ranking collection. This stemmed from the rate function not complying with an exponential curve when using sampled data that is not sufficiently ordered. This can be observed by considering a situation where the probabilistic sampled data fluctuates as we descend down the rankings. We would always expect succeeding values to be less than or equal too the previous value.

\section{Comparing Methods} \label{compMethods}

Comparing our new methods to existing methods we can evaluate how well our methods are doing. We will use the Sheffield run data from the CLEF 2017 task \cite{Kanoulas12017}.

\begin{table}[H]

\centering
\begin{tabular}{|c|c|c|c|c|} 
 \hline
 Method & Target & Recall & Reliability & Effort  \\ 
 Knee Method & - & 0.88 & 0.86 & 0.64 \\
 Target Method & 10 & 0.95 & 0.96 & 0.65 \\
 Sheffield-run2-curve & - & 0.75 & 0.64 & 0.51 \\
 Sheffield-run2-cutoff(85.5\%) & - & 0.93 & 1.0 & 0.53 \\
 Sheffield-run2-poisson & - & 0.90 & 1.0 & 0.67 \\
 Sheffield-run2-poisson + cutoff(85.5\%) & - & 0.89 & 0.95 & 0.54 \\
 \hline
\end{tabular}
\caption{Comparing target method, knee, cut-off and curve fitting along with confidence interval. Using Sheffield-run-2}

\end{table}

As the target method allows us to specify our level of reliability, we needed a target $T$ of 10 to hit 95\% reliability. We can see the cut off method alone does quite well, however the limitation of this approach is that we had to learn the best cut-off point (85.5\%). The Poisson process does well, however the effort is still high, due to the number of topics that could not be ran.

Finally, we include an ensemble solution of both the Poisson process and the cut-off method. For topics that failed during the Poisson process stage we apply the cut-off method. For the Sheffield rankings this meant 14 topics went through the Poisson process and the remaining 9 went through the cut-off method. 

Overall we believe that the Knee and Target methods are too sensitive to the ranking algorithm being used. On our Sheffield-run2 rankings we can see the performance for both these methods is significantly lower than that reported in previous work \cite{Cormack2016}. The cut-off method, whilst very simple does well in comparison to the other methods, but is limited by having to find the optimum cut-off rate. The Poisson process shows promise, but needs very tweaking and optimization to be further improved. 

\section{Automatic Full Text Retrieval} \label{automatic_f_t_r}


A common theme throughout all of the methods presented above is that we actually using very little information to find a stopping point. What would be useful is to use the abstracts and full texts to as a way of determining relevant and non-relevant documents. Abstracts call easily be retrieved by mapping the study ids in the rankings to a PubMed query. Full texts are much more challenging to retrieve due to restrictions in licences and general availability. This final section provides some initial work on retrieving full texts automatically and evaluating the retrieval rate.

We developed an experiment that uses the qrel file from the CLEF 2017 task. The qrel contains a list a studies for each Cochrane systematic review, as-well as a flag indicating whether or not it was used in the finished review. We will first read this qrel file into memory and send a request to the following resources in attempt to retrieve the pdf:

\begin{itemize}
\item Springer Link
\item Humana Press
\item Blackwell Synergy
\item Wiley
\item Science Direct
\item Choose Science Direct
\item Ingenta Connect
\item Cell Press
\item jbc
\item Nature
\item Nature Reviews
\item Pubmed Central
\item PNAS
\item Cold Spring Harbour

\end{itemize}

\begin{table}[H]

\centering
\begin{tabular}{|c|c|c|c|} 
\hline
 Type & Total Number in qrel[50 P/R Max] & PDFs retrieved & Rate  \\ 
 Relevant Documents & 607 &		213 & 0.35 \\ 
 Non-Relevant Documents & 1500 &		436 & 0.29 \\ 
 Both & 2107  &		2107 & 0.30 \\ 

 \hline
\end{tabular}

\caption{Return rate of PDF studies for 30 systematic reviews using content-level qrel file}

\end{table}

As the the number of Non-Relevant studies was exceedingly high (Over 120000 for all reviews), we limited the number for each study to 50. 

We are able to acquire  30\% of the pdfs through automated techniques. Our return rate relevant documents is slightly higher, this is beneficial as the information is much more useful.


\section{Chapter Summary}

This section presented some new approaches to finding stopping points for the data filtering stage of the systematic review process. We found approaches that use sampling to have strong potential, but challenging to develop due to having to find an optimum sample size and varying quality in data rankings. We found two methods that only need to transverse down the document ranking set and do not require sampling to actually provide good results. We also looked at how we can acquire new information from the rankings by obtaining full texts and/or abstracts.
